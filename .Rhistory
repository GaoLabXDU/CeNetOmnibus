test=seq(dim(trainsample)[1]+1,dim(testsample)[1])
DNN(input.dimention = input.dimention,output.dimention = output.dimention,layers = layer,dimention = dimention,input = input,output = output,
trainsample = train,testsample = test,learning_rate = 0.001,batch_size = 100,drop.prob = 1)
library(tensorflow)
datasets <- tf$contrib$learn$datasets
mnist <- datasets$mnist$read_data_sets("MNIST-data", one_hot = TRUE)
trainsample=mnist$train$images
trainlabel=mnist$train$labels
testsample=mnist$test$images
testlabel=mnist$test$labels
input.dimention=784
output.dimention=10
layer=1
dimention=c(784,784,784)
input=rbind(trainsample,testsample)
output=rbind(trainlabel,testlabel)
train=seq(1,dim(trainsample)[1])
test=seq(dim(trainsample)[1]+1,dim(testsample)[1])
DNN(input.dimention = input.dimention,output.dimention = output.dimention,layers = layer,dimention = dimention,input = input,output = output,
trainsample = train,testsample = test,learning_rate = 0.001,batch_size = 100,drop.prob = 1)
DNN=function(input.dimention,output.dimention,layers,dimention,input,output,trainsample,testsample,all.valid.motif,learning_rate=0.01,batch_size=100,drop.prob=0.5)
{
library(tensorflow)
tf$reset_default_graph()
x=tf$placeholder(dtype = tf$float32,shape = shape(NULL,input.dimention),name = "input")
in.d=input.dimention
in.v=x
for(i in seq(1,layers))
{
W=tf$Variable(tf$random_normal(shape = shape(in.d,dimention[i])),name = paste("weight",i,sep=''))
tf$add_to_collection(name = "loss",value = tf$contrib$layers$l2_regularizer(0.01)(W))
B=tf$Variable(tf$random_normal(shape = shape(dimention[i])),name = paste("bias",i,sep=''))
ou.v=tf$add(x = tf$matmul(in.v,W),y=B)
ou.v=tf$nn$dropout(x = ou.v,keep_prob = drop.prob)
ou.v=tf$nn$sigmoid(ou.v,name = paste("out",i,sep=''))
in.d=dimention[i]
in.v=ou.v
tf$summary$histogram(name = paste("weight",i,sep=''),values = W)
#tf$summary$image(name = paste("weight",i,sep=''),tensor = W,max_outputs = 2)
}
tf$summary$histogram(name=paste("last output"),values = in.v)
W=tf$Variable(tf$random_normal(shape = shape(in.d,output.dimention)),name="weight_out")
#tf$add_to_collection(name="loss",value = tf$contrib$layers$l2_regularizer(0.01)(W))
B=tf$Variable(tf$random_normal(shape = shape(output.dimention)),name="bias_out")
y=tf$nn$softmax(tf$add(x = tf$matmul(in.v,W),y=B),name="out")
#y=tf$nn$softmax(y)
tf$summary$histogram(name = "weighto",values = W)
y_=tf$placeholder(dtype = tf$float32,shape=shape(NULL,output.dimention))
cross_entropy = -tf$reduce_sum(y_*tf$log(tf$clip_by_value(y,1e-10,1.0)))
tf$add_to_collection(name="loss",value = cross_entropy)
loss=tf$add_n(tf$get_collection(key = "loss"))
tf$summary$scalar(name = "cross_entropy",tensor = cross_entropy)
tf$summary$scalar(name = "loss", tensor = loss)
optimal=tf$train$AdamOptimizer(learning_rate)
train_step=optimal$minimize(cross_entropy)
correct_prediction <- tf$equal(tf$argmax(y, 1L), tf$argmax(y_, 1L))
accuracy <- tf$reduce_mean(tf$cast(correct_prediction, tf$float32))
merge_summary=tf$summary$merge_all()
init <- tf$global_variables_initializer()
sess=tf$Session()
writer = tf$summary$FileWriter(logdir = "test",graph = tf$get_default_graph())
sess$run(init)
next_batch=function(input,output,size)
{
resample=sample(trainsample)
indexes=resample[1:size]
feature=input[indexes,]
label=output[indexes,]
return(list(x=feature,y_=label))
}
# datasets <- tf$contrib$learn$datasets
# mnist <- datasets$mnist$read_data_sets("MNIST-data", one_hot = TRUE)
base_batches=next_batch(input,output,batch_size)
print(c(sum(output[testsample,1]),sum(output[testsample,2]),sum(output[testsample,3]),sum(output[testsample,4])))
for(i in seq(1,1000000))
{
batches <- next_batch(input,output,batch_size)
re=sess$run(c(train_step,cross_entropy,merge_summary),feed_dict=dict(x=batches[["x"]],y_=batches[["y_"]]))
writer$add_summary(re[[3]],i)
#re=sess$run(c(train_step,cross_entropy),feed_dict=dict(x=batches[[1]],y_=batches[[2]]))
#print(paste("Iteration ",i,", cross entropy:",re[[2]],sep=''))
if(i%%100==0)
{
#re1=sess$run(cross_entropy,feed_dict = dict(x=base_batches[['x']],y_=base_batches[['y_']]))
re=sess$run(c(cross_entropy,accuracy,tf$argmax(y,1L),tf$argmax(y_,1L),y),feed_dict = dict(x=input[testsample,],y_=output[testsample,]))
pre=re[[3]]
real=re[[4]]
m=matrix(0,nrow=4,ncol=4)
for(a in seq(1,4))
{
for(b in seq(1,4))
{
m[a,b]=length(which(real==a-1&pre==b-1))
}
}
print(head(re[[5]]))
re2=sess$run(c(cross_entropy,accuracy),feed_dict=dict(x=base_batches[['x']],y_=base_batches[['y_']]))
print(paste("Iteration:",i,"loss: ",re[[1]],"ACC:",re[[2]]))
print(m)
print(paste("Iteration:",i,"base batch loss: ",re2[[1]],"ACC:",re2[[2]]))
}
if(i%%10000==0)
{
re=sess$run(accuracy,feed_dict=dict(x=input[trainsample,],y_=output[trainsample,]))
print(paste("Iteration ",i, "Training set accuracy ",re[[1]],sep=""))
}
#rownames(re[[1]])=rownames(batches[["x"]])
#print(head(re[[1]]))
}
writer$close()
}
DNN(input.dimention = input.dimention,output.dimention = output.dimention,layers = layer,dimention = dimention,input = input,output = output,
trainsample = train,testsample = test,learning_rate = 0.001,batch_size = 100,drop.prob = 1)
dimention=c(10,784,784)
input=rbind(trainsample,testsample)
output=rbind(trainlabel,testlabel)
train=seq(1,dim(trainsample)[1])
test=seq(dim(trainsample)[1]+1,dim(testsample)[1])
DNN(input.dimention = input.dimention,output.dimention = output.dimention,layers = layer,dimention = dimention,input = input,output = output,
trainsample = train,testsample = test,learning_rate = 0.001,batch_size = 100,drop.prob = 1)
View(testlabel)
library(cluster)
data(ruspini)
pr4 <- pam(ruspini, 4)
str(si <- silhouette(pr4))
(ssi <- summary(si))
plot(si) # silhouette plot
plot(si, col = c("red", "green", "blue", "purple"))# with cluster-wise coloring
dim(ruspini)
View(ruspini)
View(pr4)
class(pr4)
pr4$clustering
silhouette(pr4$clusinfo)
silhouette(pr4$clusinfo)
pr4$clusinfo
pr4$clustering
silhouette(pr4$clustering)
pr4$diss
class(pr4$diss)
silhouette(pr4$clustering,pr4$diss)
plot(silhouette(pr4$clustering,pr4$diss))
class(pr4$clustering)
typeof(pr4$clustering)
summary(silhouette(pr4$clustering,pr4$diss))
t=summary(silhouette(pr4$clustering,pr4$diss))
t$si.summary
View(t)
mean(t$clus.avg.widths)
chooseCRANmirror(local.only = T)
chooseBioCmirror(local.only = T)
library(BiocInstaller)
biocLite('TCGAbiolinksGUI')
biocLite('TCGAbiolinksGUI')
chooseCRANmirror(local.only = T)
chooseBioCmirror()
library(BiocInstaller)
biocLite('TCGAbiolinksGUI')
sessionInfo()
library(TCGAbiolinksGUI)
chooseCRANmirror(local.only = T)
install.packages('shinydashboard')
library(TCGAbiolinksGUI)
install.packages('GetoptLong')
library(TCGAbiolinksGUI)
remove.packages("TCGAbiolinksGUI")
remove.packages("TCGAbiolinks")
chooseBioCmirror(local.only = T)
library(BiocInstaller)
biocLite('TCGAbiolinksGUI')
biocLite('TCGAbiolinksGUI')
library(installr)
library(BiocInstaller)
chooseBioCmirror(local.only = T)
chooseCRANmirror(local.only = T
)
update.packages()
biocLite('TCGAbiolinksGUI')
chooseCRANmirror(local.only = )
chooseCRANmirror(local.only = T)
install.packages('installr')
chooseCRANmirror(local.only = T)
install.packages('installr')
library(installr)
update()
chooseCRANmirror(local.only = T)
install.packages('shiny')
install.packages('shinydashboard')
install.packages('shinydWidget')
install.packages('shinydwidget')
install.packages('shinywidget')
install.packages('shinyWidget')
install.packages('shinyWidgets')
library(parallel)
library(biomaRt)
library(shiny)
library(plyr)
library(ggplot2)
library(jsonlite)
library(shinydashboard)
library(shinyWidgets)
library(DT)
library(ggthemr)
chooseBioCmirror(local.only = # Tue Sep 24 01:27:25 2019 ------------------------------
)
chooseBioCmirror(local.only = T)
library("BiocInstaller", lib.loc="D:/Program Files/R/R-3.6.1/library")
chooseBioCmirror(local.only = T)
devtools::install_github('cttobin/ggthemr')
install.packages('devtools')
devtools::install_github('cttobin/ggthemr')
update.packages()
if (!requireNamespace("BiocManager", quietly = TRUE))
install.packages("BiocManager")
BiocManager::install()
chooseCRANmirror(local.only = T)
chooseBioCmirror(local.only = T)
update.packages()
BiocManager::install(update=TRUE, ask=FALSE)
shiny::runApp('D:/程序/ceRNA-Analysis/app')
shiny::runApp('D:/程序/ceRNA-Analysis/app')
sect_output_geneinfo
sect_output_geneinfo
View(groupcounts)
View(pairchoice)
unique(pairchoice)
shiny::runApp('D:/程序/ceRNA-Analysis/app')
shiny::runApp('D:/程序/ceRNA-Analysis/app')
shiny::runApp('D:/程序/ceRNA-Analysis/app')
shiny::runApp('D:/程序/ceRNA-Analysis/app')
runApp('D:/程序/ceRNA-Analysis/app')
shiny::runApp('D:/程序/ceRNA-Analysis/app')
6.405-0.2*0+3.707*0-2.311*0+0.374*10
6.405-0.2*1+3.707*0-2.311*0+0.374*10
6.405-0.2*0+3.707*1-2.311*0+0.374*10
6.405-0.2*1+3.707*1-2.311*1+0.374*10
13.852-11.341
10.145-9.945
data=read.table(file = "C:\Users\william\Desktop\facebook-links.txt.anon",header=F,stringsAsFactors=F)
data=read.table(file = "C:/Users/william/Desktop/facebook-links.txt.anon",header=F,stringsAsFactors=F)
head(data)
data=data[which(data$V3!="\\N")]
data=data[which(data$V3!="\\N"),]
640121+905565
write.table(x = data,file = "after_facebook.txt",quote = F,sep = "\t",row.names = F,col.names = F)
library(R.matlab)
#projName = NULL
#tmpdir=NULL
#typeLimit = 10
Super <- function(run = TRUE,maxRequestSize=5*1024^2,workpath=tempdir(),projectName=NULL,typeLimit=10,...) {
library(parallel)
library(biomaRt)
library(shiny)
library(plyr)
library(ggplot2)
library(jsonlite)
library(shinydashboard)
library(shinyWidgets)
library(DT)
library(ggthemr)
library(R.matlab)
library(tibble)
library(igraph)
library(scales)
library(rhandsontable)
library(PerformanceAnalytics)
#library(rJava)
library(linkcomm)
library(MCL)
library(visNetwork)
library(colourpicker)
#library(ProNet)
library(ggplotify)
library(survival)
library(survminer)
library(ComplexHeatmap)
library(circlize)
tmpdir<<-normalizePath(workpath)
projName <<- projectName
typeLimit <<- typeLimit
ggthemr('flat')
usedcolors=swatch()
options(shiny.maxRequestSize = maxRequestSize)
if(run) suppressMessages(shiny::runApp(system.file("app", package = "shinyAppDemo"),launch.browser=TRUE,...))
}
Super()
remotes::install_github("GuangchuangYu/nCov2019")
library("nCov2019", lib.loc="D:/Program Files/R/R-3.6.1/library")
data=get_nCov2019()
data$areaTree
data$chinaTotal
data$chinaDayList
t=data$areaTree
View(t)
class(t)
data$chinaDayAddList
t=data$lastUpdateTime
t=data$areaTree
data$areaTree
data$areaTree$children
data$areaTree$children[[1]]
t=data$areaTree$children[[1]]
t=data$areaTree$children[[1]]$[[22]]
t=data$areaTree$children[[1]]$children[[22]]
get_nCov2019
.get_json
data[]
class(data)
summary(data)
library(RCurl)
getURL("www.baidu.com")
getURL("www.google.com")
getURL("www.google.com")
library(biomaRt)
data=data.frame(a=c(1,2,3),b=c(2,3,4))
data
rep(data[1,],times=times)
rep(data[1,],times=3)
t=rep(data[1,],times=3)
log2(2e-4)
107/3500
11*/160
11/160
log(2e-4)
log2(2e-4)
library(devtools)
!'a' %in% c('a','b')
document()
library(devtools)
document()
devtools::install_cran
library(devtools)
document()
0.155/8e-4-12e-8/3.6e-8-0.05/0.8e-4
kmer=function(seq,k)
{
seq=""
vec=c('A','T','C','G')
for(i in seq(1,k))
{
seq=paste(rep(seq,times=length(vec)),rep(vec,each=length(seq)),sep="")
}
return(seq)
}
kmer=function(k) {   seq=""   vec=c('A','T','C','G')   for(i in seq(1,k))   {     seq=paste(rep(seq,times=length(vec)),rep(vec,each=length(seq)),sep="")   }   return(seq) }
kmer=function(k)
{
seq=""
vec=c('A','T','C','G')
for(i in seq(1,k))
{
seq=paste(rep(seq,times=length(vec)),rep(vec,each=length(seq)),sep="")
}
return(seq)
}
kmer(5)
a <- "aggcacggaaaaacgggaataacggaggaggacttggcacggcattacacggagg"
gregexpr("ag",a)
gregexpr("ag",c(a,a))
toupper(a)
mer=mers[1]
kmer=function(k)
{
seq=""
vec=c('A','T','C','G')
for(i in seq(1,k))
{
seq=paste(rep(seq,times=length(vec)),rep(vec,each=length(seq)),sep="")
}
return(seq)
}
mers=kerm(5)
kmer=function(k)
{
seq=""
vec=c('A','T','C','G')
for(i in seq(1,k))
{
seq=paste(rep(seq,times=length(vec)),rep(vec,each=length(seq)),sep="")
}
return(seq)
}
mers=kmer(5)
mer=mers[1]
mathch=gregexpr(mer,seqs)
seqs=c('AGGCACGGAAAAACGGGAATAACGGAGGAGGACTTGGCACGGCATTACACGGAGG','AGGCACGGAAAAACGGGAATAACGGAGGAGGACTTGGCACGGCATTACACGGAGG')
mathch=gregexpr(mer,seqs)
attr(match, "match.length")
lapply(X = match,FUN = attr,which="match.length")
attr(match[[1]],which = 'match.length')
match[[1]]
match[1
]
View(mathch)
View(mathch[[1]])
match=gregexpr(mer,seqs)
lapply(X = match,FUN = attr,which="match.length")
kmer=function(k)
{
seq=""
vec=c('A','T','C','G')
for(i in seq(1,k))
{
seq=paste(rep(seq,times=length(vec)),rep(vec,each=length(seq)),sep="")
}
return(seq)
}
mers=kmer(5)
seqs=c('AGGCACGGAAAAACGGGAATAACGGAGGAGGACTTGGCACGGCATTACACGGAGG','AGGCACGGAAAAACGGGAATAACGGAGGAGGACTTGGCACGGCATTACACGGAGG')
for(mer in mers)
{
match=gregexpr(mer,seqs)
times=unlist(lapply(X = match,FUN = attr,which='match.length'))
print(paste(mer,times))
}
gregexpr(pattern = 'CACGG',text = 'AGGCACGGAAAAACGGGAATAACGGAGGAGGACTTGGCACGGCATTACACGGAGG')
t=gregexpr(pattern = 'CACGG',text = 'AGGCACGGAAAAACGGGAATAACGGAGGAGGACTTGGCACGGCATTACACGGAGG',)
attr(t,'match.length')
t
attr(t[[1]],'match.length')
kmer=function(k)
{
seq=""
vec=c('A','T','C','G')
for(i in seq(1,k))
{
seq=paste(rep(seq,times=length(vec)),rep(vec,each=length(seq)),sep="")
}
return(seq)
}
getResult=function(x)
{
return(length(attr(x,'match.length')))
}
mers=kmer(5)
seqs=c('AGGCACGGAAAAACGGGAATAACGGAGGAGGACTTGGCACGGCATTACACGGAGG','AGGCACGGAAAAACGGGAATAACGGAGGAGGACTTGGCACGGCATTACACGGAGG')
for(mer in mers)
{
match=gregexpr(mer,seqs)
times=unlist(lapply(X = match,FUN = getResult))
}
times
mer='CACGG'
match=gregexpr(mer,seqs)
times=unlist(lapply(X = match,FUN = getResult))
times
gregexpr(pattern = 'atata',text = 'ata')
gregexpr(pattern = 'ata',text = 'atata')
gregexpr(pattern = 'ata',text = 'atata',fixed = T)
gregexpr(pattern = 'ata',text = 'atata',fixed = F)
gregexpr(pattern = 'aa',text = 'aaaaa')
BiocManager::install('WGCNA')
library(CeNetOmnibus)
CeNetOmnibus()
CeNetOmnibus()
library(CeNetOmnibus)
CeNetOmnibus()
CeNetOmnibus()
library(CeNetOmnibus)
CeNetOmnibus()
data=readRDS('C:/Users/william/Desktop/GSM4557339_HIP045_cell.counts.matrices.rds')
library(Matrix)
data=readRDS('C:/Users/william/Desktop/GSM4557339_HIP045_cell.counts.matrices.rds')
data$exon
data=readRDS('C:/Users/william/Desktop/GSM4557339_HIP045_cell.counts.matrices.rds')
class(data)
class(data$exon)
class(data$intron)
class(data$spanning)
t=as.matrix(x = data$exon)
dim(t)
rownames(t)
colnames(t)
tt=as.matrix(data$intron)
dim(t)
dim(tt)
rownames(tt)
dim(t)
dim(tt)
ttt=as.matrix(data$spanning)
dim(ttt)
rownames(ttt)
t[1,]
which(rowSums(t)!=0)
data=readRDS('C:/Users/william/Desktop/GSM4557339_HIP045_cell.counts.matrices.rds')、
data=readRDS('C:/Users/william/Desktop/GSM4557339_HIP045_cell.counts.matrices.rds')
data1=as.matrix(data$exon)
data2=as.matrix(data$intron)
data3=as.matrix(data$spanning)
dim(data1)
dim(data2)
dim(data3)
rownames(data3)
name=rownames(data1)
grep(pattern = "mir",x = name)
name[grep(pattern = "mir",x = name)]
data1[name[grep(pattern = "mir",x = name)],]
t=t(data1[name[grep(pattern = "mir",x = name)],])
View(t)
all(t==0)
which(t!=0)
library(devtools)
install_github('william0701/ceNetOmnibus-release')
install_github('william0701/ceNetOmnibus-release')
install_github('william0701/ceNetOmnibus-release')
setwd("D:/程序/ceNetOmnibus-release")
remove.packages(c("shinydashboard","shinyWidgets","rhandsontable","PerformanceAnalytics","linkcomm","MCL","visNetwork","colourpicker","ggplotify","survminer","circlize","formattable","infotheo","gprofiler2","ggsci","ggsignif","polynom","rstatix","broom","corrplot","car","carData","abind","pbkrtest","quantreg","maptools","rio","lme4","minqa","nloptr","statmod","sp","SparseM","MatrixModels","haven","readxl","openxlsx","forcats","hms","readr","zip","cellranger","progress","rematch","exactRankTests","mvtnorm","KMsurv","km.ci","tinytex","xts","quadprog","dynamicTreeCut","expm","miniUI","shinyjs","gridGraphics","rvcheck","ggpubr","maxstat","survMisc","GlobalOptions","shape","rmarkdown"))
devtools::install_github('william0701/CeNetOmnibus')
chooseCRANmirror()
chooseBioCmirror()
chooseCRANmirror()
devtools::install_github('william0701/CeNetOmnibus')
